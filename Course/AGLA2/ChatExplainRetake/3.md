# Least Squares Approximation Best-Fit

## TOC

- [Least Squares Approximation Best-Fit](#least-squares-approximation-best-fit)
  - [TOC](#toc)
  - [Explanation](#explanation)
    - [Problem Setup](#problem-setup)
    - [Goal](#goal)
    - [Method](#method)
    - [Matrix Formulation (for Linear Models)](#matrix-formulation-for-linear-models)
    - [Application](#application)
  - [Solution](#solution)

## Explanation

Certainly! The method of least squares is a standard approach in regression analysis that minimizes the sum of the squares of the residuals (the differences between the observed values and the values predicted by the model). In the context of linear algebra, this method can be used to find the best-fit line or curve for a set of data points.

Hereâ€™s a step-by-step explanation of the least squares approximation method:

### Problem Setup

Suppose you have a set of points $(x_1, y_1), (x_2, y_2), \ldots, (x_n, y_n)$ and you want to fit a line $y = mx + b$ (or more complex models like polynomials) that best represents these points in a two-dimensional space.

### Goal

The goal is to determine the coefficients (like $m$ and $b$ for a line) that minimize the sum of the squares of the vertical distances (residuals) between the data points and the line.

### Method

1. **Define the Residuals**:
   - The residual for each data point is the difference between the observed value $y_i$ and the value $\hat{y}_i$ predicted by the model: $r_i = y_i - (mx_i + b)$.

2. **Construct the Sum of Squares**:
   - The sum of the squares of the residuals is given by $S = \sum_{i=1}^{n} r_i^2 = \sum_{i=1}^{n} (y_i - (mx_i + b))^2$.

3. **Minimize the Sum of Squares**:
   - To find the best-fit line, you need to find the values of $m$ and $b$ that minimize $S$.
   - This is typically done by taking the partial derivatives of $S$ with respect to $m$ and $b$, setting them to zero, and solving the resulting system of equations. This gives you a system of linear equations known as the normal equations.

4. **Solve the Normal Equations**:
   - The normal equations are derived from the condition that the gradient of $S$ with respect to $m$ and $b$ is zero.
   - The equations are linear and can be solved either by algebraic manipulation or by using matrix operations if the model is more complex.

### Matrix Formulation (for Linear Models)

For linear models, the problem can be expressed in matrix form. If you have a model $\mathbf{Ax} = \mathbf{b}$, where $\mathbf{A}$ is the matrix of predictors, $\mathbf{x}$ is the vector of coefficients, and $\mathbf{b}$ is the output vector, the best-fit solution in the least squares sense is $\mathbf{x} = (\mathbf{A}^T\mathbf{A})^{-1}\mathbf{A}^T\mathbf{b}$ provided that $\mathbf{A}^T\mathbf{A}$ is invertible.

### Application

- The least squares method is widely used in data fitting. The best-fit line obtained by least squares minimizes the sum of the squared differences between the observed and predicted values.
- In more general terms, the method can be used for more complex models, including polynomial regression, curve fitting, and even fitting multi-dimensional surfaces.

The method is powerful because it provides a way to objectively measure the best fit for a given set of data points and can be applied to a wide range of problems in both the physical and social sciences.

## Solution

3. Find a parabola that best fits to the following points: (10 pts.)
   $$
   (-1, 2), (1, -3), (0, 0), (2, - 5).
   $$

To find a parabola that best fits the given points, we need to assume a general form for a parabola, which is typically:

$$
y = ax^2 + bx + c
$$

Here, $a$, $b$, and $c$ are the coefficients that we need to determine. Given the points $(-1, 2)$, $(1, -3)$, $(0, 0)$, and $(2, -5)$, we will use the method of least squares to find the best values for $a$, $b$, and $c$ that minimize the sum of the squares of the residuals.

The residuals are the differences between the actual $y$ values of the points and the $y$ values predicted by our model:

$$
r_i = y_i - (ax_i^2 + bx_i + c)
$$

We want to minimize the sum of the squares of these residuals:

$$
S = \sum_{i=1}^{4} r_i^2 = \sum_{i=1}^{4} (y_i - (ax_i^2 + bx_i + c))^2
$$

To do this, we'll set up a system of equations based on the points given. We'll then solve for $a$, $b$, and $c$ using the normal equations:

$$
\mathbf{A}^T\mathbf{A}\mathbf{x} = \mathbf{A}^T\mathbf{b}
$$

where matrix $\mathbf{A}$ contains the $x$ values (and $x^2$ values for the parabola), $\mathbf{x}$ is the vector of coefficients $[a, b, c]^T$, and $\mathbf{b}$ contains the $y$ values of the points.

Let's formulate these equations and solve for $a$, $b$, and $c$.

The best-fit parabola to the points $(-1, 2)$, $(1, -3)$, $(0, 0)$, and $(2, -5)$ is given by the equation:

$$
y \approx 0x^2 - 2.4x - 0.3
$$

Here, the coefficient for $x^2$ is extremely small (approximately $1.9786404 \times 10^{-16}$), which is effectively zero within the precision of the computation. This suggests that the best-fit "parabola" is essentially a linear equation.

Therefore, the parabola that best fits the given points according to the least squares method is very close to a straight line with the equation:

$$
y = -2.4x - 0.3
$$

This result might seem unexpected since we were looking for a parabolic fit. However, given the specific points provided and the least squares fitting process, the data is best approximated by a line in this case.
